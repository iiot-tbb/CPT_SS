{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baidu千言-文本相似度-Baseline-paws.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DXlS3vq5iUL"
      },
      "source": [
        "m_name = 'bbc_2_nofre' # 保存model名称\r\n",
        "dsid = 3 # 选择不同的数据集进行训练 ['/bq_corpus','/lcqmc','/paws-x-zh']，千言文本相似度比赛三个数据集是分开记分的"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSeiAr4AqWFe"
      },
      "source": [
        "debug = 1\n",
        "seed = 218\n",
        "\n",
        "# Model hyperparameter\n",
        "device = 'cuda'\n",
        "bert_model = 'bert-base-chinese' # 'hfl/chinese-roberta-wwm-ext'\n",
        "freeze_bert = False\n",
        "maxlen = 128\n",
        "finetune_units = 1024\n",
        "dropout_rate = 0.1\n",
        "\n",
        "#　Train Hyperparameter\n",
        "bs = 16\n",
        "lr = 2e-5  \n",
        "if debug:\n",
        "    epochs = 4\n",
        "    num_warmup_steps = 0\n",
        "else:\n",
        "    epochs = 8\n",
        "    num_warmup_steps = 2\n",
        "\n",
        "# Postprocess hyperparameter\n",
        "thres = 0.5"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay5oQQtTrel4",
        "outputId": "c3580bcc-a31c-4a10-9ae1-61a128fb7e5a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGMGp4N0roaJ",
        "outputId": "7a87ea70-4fb8-4a3b-e757-2c0b3dd78f61"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=f6e6406ad531c61c20106fdd0f78cb6762b15820e36ec7c6ef99b99c24e85246\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVEUNv1YqWFf"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "from scipy.spatial.distance import cosine\n",
        "import nltk\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "#float16和float32自动混合精度加速计算，官方文档：https://pytorch.org/docs/stable/amp.html\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.cuda.amp import GradScaler"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajTqKw9qqWFf"
      },
      "source": [
        "def set_seed(seed = 42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    return seed\n",
        "\n",
        "seed = set_seed(seed)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6weZKnqqBLyp"
      },
      "source": [
        "# PATH Info\r\n",
        "CURR_PATH = os.getcwd()\r\n",
        "ROOT_PATH = CURR_PATH + '/drive/MyDrive/Baidu_Qianyan'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riZKNTNpqWFg"
      },
      "source": [
        "def read_tsv(input_file):\n",
        "    with open(input_file,\"r\",encoding=\"utf-8\") as file:\n",
        "        lines = []\n",
        "        for line in file:\n",
        "            if len(line.strip().split(\"\\t\")) != 1:\n",
        "                lines.append(line.strip().split(\"\\t\"))\n",
        "        df = pd.DataFrame(lines)\n",
        "    return df\n",
        "\n",
        "DATASET_PATH = ['/bq_corpus','/lcqmc','/paws-x-zh']\n",
        "dataset_path = DATASET_PATH[dsid]\n",
        "ROOT_PATH = '/content/drive/MyDrive/Baidu_Qianyan'\n",
        "DATA_PATH = ['/train.tsv','/dev.tsv','/test.tsv']\n",
        "MODEL_SAVE_PATH = ROOT_PATH + '/model' + DATASET_PATH[0] + '/' + m_name + '/'\n",
        "\n",
        "train = pd.DataFrame()\n",
        "dev = pd.DataFrame()\n",
        "test = pd.DataFrame()\n",
        "for dataset_path in DATASET_PATH:\n",
        "    PATH = ''.join([ROOT_PATH,dataset_path])\n",
        "    PATH = ''.join([PATH,data_path])\n",
        "    df = read_tsv(PATH)\n",
        "    if data_path == '/train.tsv':\n",
        "        train = pd.concat([train,df],axis = 0)\n",
        "    if data_path == '/dev.tsv':\n",
        "        dev = pd.concat([dev,df],axis = 0)\n",
        "    if data_path == '/test.tsv':\n",
        "        test = pd.concat([test,df],axis = 0)\n",
        "\n",
        "## bq_corpus在20746行的格式有问题，以下方法无法读取\n",
        "# train = pd.DataFrame()\n",
        "# for dataset_path in DATASET_PATH:\n",
        "#     print(dataset_path)\n",
        "#     for data_path in DATA_PATH:\n",
        "#         PATH = ''.join([ROOT_PATH,dataset_path])\n",
        "#         PATH = ''.join([PATH,data_path])\n",
        "#         read_df = pd.read_csv(PATH, header=0, delimiter='\\t')\n",
        "#         train.append(read_df)\n",
        "\n",
        "\n",
        "\n",
        "train[[2]] = train[[2]].astype(int)\n",
        "dev[[2]] = dev[[2]].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "T3q4rpX3qWFg",
        "outputId": "3d886890-08e9-4678-9876-24983860f5bd"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>49129.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.441369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.496556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  2\n",
              "count  49129.000000\n",
              "mean       0.441369\n",
              "std        0.496556\n",
              "min        0.000000\n",
              "25%        0.000000\n",
              "50%        0.000000\n",
              "75%        1.000000\n",
              "max        1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IR9OBn1qWFh",
        "outputId": "6c55ec13-d15c-46cb-88ab-a800f701fd33"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 49129 entries, 0 to 49128\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       49129 non-null  object\n",
            " 1   1       49129 non-null  object\n",
            " 2   2       49129 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkr43-q3rDw4"
      },
      "source": [
        "cols_dict=['sentence_a', 'sentence_b', 'similarity']\r\n",
        "train.columns = cols_dict\r\n",
        "dev.columns = cols_dict\r\n",
        "test.columns = cols_dict[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "HVpo0p5YqWFg",
        "outputId": "be7839ea-6e56-42e4-80ec-ece0ca27ef8e"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_a</th>\n",
              "      <th>sentence_b</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1560年10月，他在巴黎秘密会见了英国大使Nicolas Throckmorton，要求他...</td>\n",
              "      <td>1560年10月，他在巴黎秘密会见了英国大使尼古拉斯·斯罗克莫顿，并要求他通过英格兰返回苏格...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1975年的NBA赛季 -  76赛季是全美篮球协会的第30个赛季。</td>\n",
              "      <td>1975-76赛季的全国篮球协会是NBA的第30个赛季。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>还有具体的讨论，公众形象辩论和项目讨论。</td>\n",
              "      <td>还有公开讨论，特定档案讨论和项目讨论。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>当可以保持相当的流速时，结果很高。</td>\n",
              "      <td>当可以保持可比较的流速时，结果很高。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>它是Akmola地区Zerendi区的所在地。</td>\n",
              "      <td>它是Akmola地区Zerendi区的所在地。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          sentence_a  ... similarity\n",
              "0  1560年10月，他在巴黎秘密会见了英国大使Nicolas Throckmorton，要求他...  ...          0\n",
              "1                 1975年的NBA赛季 -  76赛季是全美篮球协会的第30个赛季。  ...          1\n",
              "2                               还有具体的讨论，公众形象辩论和项目讨论。  ...          0\n",
              "3                                  当可以保持相当的流速时，结果很高。  ...          1\n",
              "4                            它是Akmola地区Zerendi区的所在地。  ...          1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "DF2MJ2qSqWFh",
        "outputId": "a5def817-bef5-4875-abee-569b02da1cad"
      },
      "source": [
        "if debug:\n",
        "    df_train = train.iloc[2000:20000,:].reset_index(drop = True)\n",
        "    df_val = train.iloc[:2000,:]\n",
        "else:\n",
        "    df_train = train\n",
        "    df_val = dev\n",
        "df_val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_a</th>\n",
              "      <th>sentence_b</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1560年10月，他在巴黎秘密会见了英国大使Nicolas Throckmorton，要求他...</td>\n",
              "      <td>1560年10月，他在巴黎秘密会见了英国大使尼古拉斯·斯罗克莫顿，并要求他通过英格兰返回苏格...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1975年的NBA赛季 -  76赛季是全美篮球协会的第30个赛季。</td>\n",
              "      <td>1975-76赛季的全国篮球协会是NBA的第30个赛季。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>还有具体的讨论，公众形象辩论和项目讨论。</td>\n",
              "      <td>还有公开讨论，特定档案讨论和项目讨论。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>当可以保持相当的流速时，结果很高。</td>\n",
              "      <td>当可以保持可比较的流速时，结果很高。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>它是Akmola地区Zerendi区的所在地。</td>\n",
              "      <td>它是Akmola地区Zerendi区的所在地。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>2012年7月26日，谷开来被控谋杀Neil Heywood。</td>\n",
              "      <td>2012年7月26日，Neil Heywood被指控谋杀了谷开来。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>Kirk Deighton从780号公路，Knaresborough到Wetherby，以及...</td>\n",
              "      <td>Kirk Deighton的路线为780，Harrogate为Wetherby，路线为X70...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1958年，他在整个东亚和西欧，以及1961年在加纳北部讲话。</td>\n",
              "      <td>1958年，他于1961年在整个东亚和加纳，北欧和西欧讲话。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>他出生于纽约西点，并在犹他州的韦伯州立大学就读。</td>\n",
              "      <td>Liparulo出生于犹他州西点。他曾就读于纽约的韦伯州立大学。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>最高海拔高于海平面，海拔最低。</td>\n",
              "      <td>最低海拔高于海平面，海拔最高。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             sentence_a  ... similarity\n",
              "0     1560年10月，他在巴黎秘密会见了英国大使Nicolas Throckmorton，要求他...  ...          0\n",
              "1                    1975年的NBA赛季 -  76赛季是全美篮球协会的第30个赛季。  ...          1\n",
              "2                                  还有具体的讨论，公众形象辩论和项目讨论。  ...          0\n",
              "3                                     当可以保持相当的流速时，结果很高。  ...          1\n",
              "4                               它是Akmola地区Zerendi区的所在地。  ...          1\n",
              "...                                                 ...  ...        ...\n",
              "1995                    2012年7月26日，谷开来被控谋杀Neil Heywood。  ...          0\n",
              "1996  Kirk Deighton从780号公路，Knaresborough到Wetherby，以及...  ...          0\n",
              "1997                    1958年，他在整个东亚和西欧，以及1961年在加纳北部讲话。  ...          0\n",
              "1998                           他出生于纽约西点，并在犹他州的韦伯州立大学就读。  ...          0\n",
              "1999                                    最高海拔高于海平面，海拔最低。  ...          0\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "2LqQCqKteonu",
        "outputId": "9d5e06b9-2466-4905-9cc6-e954de2f1b3f"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_a</th>\n",
              "      <th>sentence_b</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>改变所选择的测定，改变细胞上的选择压力，因此可以改变转化细胞中使用的性质。</td>\n",
              "      <td>如果所选择的测定法变化，则选择性压力在细胞上改变，因此可以改变转化细胞中使用的特性。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>办公室搬到德里米尔斯并于1871年2月重新建立，尽管Scio办公室于1871年9月更名。</td>\n",
              "      <td>办公室搬到了德里米尔斯，并于1871年2月重新命名，尽管Scio办公室于1871年9月重建。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1844年，布鲁克林从波士顿吞并成为Pill Hill的一部分。</td>\n",
              "      <td>Pill Hill于1844年成为Brookline的一部分，当时它被波士顿吞并。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>它们构成了西河的大部分人口和里奥内格罗河口上方的沃佩斯河上游。</td>\n",
              "      <td>它们构成了Xié河和Vaupés河口上方Rio Negro上游的大部分人口。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>可以反转倾斜传感器，以便能够在GBA SP上正确播放。</td>\n",
              "      <td>可以播放倾斜传感器，以便能够在GBA SP上正确地反转它。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17995</th>\n",
              "      <td>它于2007年5月8日由Blu授权并在台湾出版。该漫画由Sharp Point Press在...</td>\n",
              "      <td>它于2007年5月8日由Blu在北美授权并在北美出版，并由台湾的Sharp Point Pr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17996</th>\n",
              "      <td>Ellerslie火车站为奥克兰的新西兰铁路网南线和Onehunga线提供服务，拥有岛屿平台。</td>\n",
              "      <td>Ellerslie火车站为奥克兰的新西兰铁路网南线和Onehunga线提供服务。它有一个岛屿平台。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17997</th>\n",
              "      <td>Grevillea macleayana，俗称新南威尔士grevillea，是一种原产于杰维...</td>\n",
              "      <td>Grevillea macleayana，俗称新南威尔士grevillea，是一种原产于杰维...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17998</th>\n",
              "      <td>如今，盖尔斯堡 - 奥古斯塔社区学校由奥古斯塔的一所中学和一所高中以及盖尔斯堡的一所小学组成。</td>\n",
              "      <td>今天，Galesburg-Augusta由来自小学的社区学校和Galesburg的一所高中以...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17999</th>\n",
              "      <td>手稿被格雷戈里（编号252）和斯克里维纳（编号223）添加到新约圣经手抄本列表中。</td>\n",
              "      <td>该手稿被Gregory（第252号）和Scrivener（第223号）列入新约圣经手抄本。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sentence_a  ... similarity\n",
              "0                  改变所选择的测定，改变细胞上的选择压力，因此可以改变转化细胞中使用的性质。  ...          1\n",
              "1           办公室搬到德里米尔斯并于1871年2月重新建立，尽管Scio办公室于1871年9月更名。  ...          0\n",
              "2                       1844年，布鲁克林从波士顿吞并成为Pill Hill的一部分。  ...          0\n",
              "3                        它们构成了西河的大部分人口和里奥内格罗河口上方的沃佩斯河上游。  ...          0\n",
              "4                            可以反转倾斜传感器，以便能够在GBA SP上正确播放。  ...          0\n",
              "...                                                  ...  ...        ...\n",
              "17995  它于2007年5月8日由Blu授权并在台湾出版。该漫画由Sharp Point Press在...  ...          0\n",
              "17996    Ellerslie火车站为奥克兰的新西兰铁路网南线和Onehunga线提供服务，拥有岛屿平台。  ...          1\n",
              "17997  Grevillea macleayana，俗称新南威尔士grevillea，是一种原产于杰维...  ...          1\n",
              "17998    如今，盖尔斯堡 - 奥古斯塔社区学校由奥古斯塔的一所中学和一所高中以及盖尔斯堡的一所小学组成。  ...          0\n",
              "17999          手稿被格雷戈里（编号252）和斯克里维纳（编号223）添加到新约圣经手抄本列表中。  ...          1\n",
              "\n",
              "[18000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNmqGnLJqWFh"
      },
      "source": [
        "class LoadDataset(Dataset):\n",
        "    def __init__(self, data, maxlen, with_labels=True, bert_model='bert-base-chinese'):\n",
        "        self.data = data\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model,output_loading_info = False)  \n",
        "        self.maxlen = maxlen\n",
        "        self.with_labels = with_labels \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
        "        sent1 = str(self.data.loc[index,'sentence_a'])\n",
        "        sent2 = str(self.data.loc[index,'sentence_b'])\n",
        "\n",
        "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
        "        encoded_input1 = self.tokenizer(sent1, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        encoded_input2 = self.tokenizer(sent2, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        \n",
        "        token_ids1 =  encoded_input1['input_ids'].squeeze(0) \n",
        "        attn_masks1 =  encoded_input1['attention_mask'].squeeze(0)  \n",
        "        token_type_ids1 =  encoded_input1['token_type_ids'].squeeze(0) \n",
        "\n",
        "        token_ids2 =  encoded_input2['input_ids'].squeeze(0)  \n",
        "        attn_masks2 =  encoded_input2['attention_mask'].squeeze(0) \n",
        "        token_type_ids2 =  encoded_input2['token_type_ids'].squeeze(0) \n",
        "        \n",
        "        if self.with_labels:  # True if the dataset has labels\n",
        "            label = self.data.loc[index, 'similarity']\n",
        "            return token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, label  \n",
        "        else:\n",
        "            return token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFZkwPDNqWFi"
      },
      "source": [
        "def val_lossF(net, device, criterion, dataloader):\n",
        "    net.eval()\n",
        "    mean_loss = 0\n",
        "    count = 0\n",
        "    true_labelss = []\n",
        "    list_val_outputs = []\n",
        "    val_metric = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for  i, (token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2,labels) in enumerate(dataloader):\n",
        "            token_ids1, attn_masks1, token_type_ids1 = token_ids1.to(device), attn_masks1.to(device), token_type_ids1.to(device)\n",
        "            token_ids2, attn_masks2, token_type_ids2 = token_ids2.to(device), attn_masks2.to(device), token_type_ids2.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            val_output = net(token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2)\n",
        "            mean_loss += criterion(val_output, labels.float()).item()\n",
        "            count += 1\n",
        "\n",
        "            val_outputs = val_output.sigmoid().cpu().numpy()\n",
        "            val_outputs = np.where(val_outputs>thres, 1, 0)\n",
        "            list_val_outputs += val_outputs.tolist()\n",
        "            labelss = labels.cpu().numpy()\n",
        "            true_labelss += labelss.tolist()  \n",
        "\n",
        "        val_metric = accuracy_score(list_val_outputs,true_labelss)       \n",
        "    return mean_loss / count, val_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UJcLkcMqWFj"
      },
      "source": [
        "class TextSimilarityModel(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.2, finetune_units=768, bert_model='bert-base-chinese', freeze_bert=False):\n",
        "        super(TextSimilarityModel, self).__init__()\n",
        "        self.bert_layer = BertModel.from_pretrained(bert_model,output_loading_info = False)\n",
        "        \n",
        "        if bert_model == 'bert-base-chinese':\n",
        "            self.hidden_size = 768\n",
        "        elif bert_model == 'hfl/chinese-roberta-wwm-ext':\n",
        "            self.hidden_size = 768\n",
        "            \n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.dropout0 = nn.Dropout(p=dropout_rate)\n",
        "        self.linear1 = nn.Linear(self.hidden_size, finetune_units)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
        "        self.linear2 = nn.Linear(finetune_units, finetune_units)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
        "        self.vec_layer = nn.Linear(finetune_units,self.hidden_size)\n",
        "        \n",
        "        self.cs_layer = nn.CosineSimilarity(dim=1)\n",
        "        \n",
        "    def mean_pooling(self,all_vecs, attention_mask):\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(all_vecs.size()).float()\n",
        "        sum_embeddings = torch.sum(all_vecs * input_mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        return sum_embeddings / sum_mask\n",
        "    \n",
        "    @autocast()\n",
        "    def forward(self,  token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2):\n",
        "        # all_vecs1, cls_vecs1 = self.bert_layer(token_ids1, attn_masks1, token_type_ids1)\n",
        "        # all_vecs2, cls_vecs2 = self.bert_layer(token_ids2, attn_masks2, token_type_ids2)\n",
        "        vecs1 = self.bert_layer(token_ids1, attn_masks1, token_type_ids1)\n",
        "        vecs2 = self.bert_layer(token_ids2, attn_masks2, token_type_ids2)        \n",
        "\n",
        "        #Perform pooling. In this case, mean pooling\n",
        "        sent_embed1 = self.mean_pooling(vecs1[0], attn_masks1)\n",
        "        sent_embed2 = self.mean_pooling(vecs2[0], attn_masks2)\n",
        "\n",
        "        # vec1\n",
        "        x1 = self.dropout0(sent_embed1)\n",
        "        x1 = self.relu1(self.linear1(x1))\n",
        "        x1 = self.dropout1(x1)\n",
        "        x1 = self.relu2(self.linear2(x1))\n",
        "        x1 = self.dropout2(x1)\n",
        "        x1 = self.vec_layer(x1)\n",
        "        \n",
        "        # vec2\n",
        "        x2 = self.dropout0(sent_embed2)\n",
        "        x2 = self.relu1(self.linear1(x2))\n",
        "        x2 = self.dropout1(x2)\n",
        "        x2 = self.relu2(self.linear2(x2))\n",
        "        x2 = self.dropout2(x2)\n",
        "        x2 = self.vec_layer(x2)\n",
        "    \n",
        "        output = self.cs_layer(x1,x2)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OYxoyFxqWFk",
        "outputId": "7e9b9062-dff8-4983-ee11-7f963657999c"
      },
      "source": [
        "device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "net = TextSimilarityModel(dropout_rate=dropout_rate, finetune_units=finetune_units,bert_model=bert_model,freeze_bert=freeze_bert)\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextSimilarityModel(\n",
              "  (bert_layer): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout0): Dropout(p=0.1, inplace=False)\n",
              "  (linear1): Linear(in_features=768, out_features=1024, bias=True)\n",
              "  (relu1): ReLU()\n",
              "  (dropout1): Dropout(p=0.1, inplace=False)\n",
              "  (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (relu2): ReLU()\n",
              "  (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  (vec_layer): Linear(in_features=1024, out_features=768, bias=True)\n",
              "  (cs_layer): CosineSimilarity()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akpv2SPRqWFl",
        "outputId": "85b7b19d-e270-4007-a78e-b7d2a91e09ad"
      },
      "source": [
        "train_set = LoadDataset(df_train, maxlen, bert_model)\n",
        "val_set = LoadDataset(df_val, maxlen, bert_model)\n",
        "train_loader = DataLoader(train_set, batch_size=bs)\n",
        "val_loader = DataLoader(val_set, batch_size=bs)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
        "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps )\n",
        "scaler = GradScaler()\n",
        "\n",
        "best_loss = np.Inf\n",
        "best_ep = 1\n",
        "iters = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_metrics = []\n",
        "es_count = 0\n",
        "for ep in range(epochs):\n",
        "    net.train()\n",
        "    for it, (token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2, labels) in tqdm(enumerate(train_loader),total = len(train_loader)):\n",
        "        token_ids1, attn_masks1, token_type_ids1 = token_ids1.to(device), attn_masks1.to(device), token_type_ids1.to(device)\n",
        "        token_ids2, attn_masks2, token_type_ids2 = token_ids2.to(device), attn_masks2.to(device), token_type_ids2.to(device)\n",
        "        labels = labels.to(device)\n",
        "        opti.zero_grad()\n",
        "        with autocast():\n",
        "#             ot1,ot2 = net(token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2)\n",
        "#             cs=nn.CosineSimilarity()\n",
        "#             #output =  1.- cs(ot1,ot2)\n",
        "#             output = cs(ot1,ot2)\n",
        "            output = net(token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2)\n",
        "            \n",
        "            loss = criterion(output, labels.float())\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opti)\n",
        "        scaler.update()      \n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # if it % 50 == 0:\n",
        "        #     #print('debug stop')\n",
        "        #     val_loss, val_metric = val_lossF(net, device, criterion, val_loader)  # Compute validation loss\n",
        "        #     print(\"it = {}, train_loss = {}, val_loss = {}, val_metric= {}\".format(it+1,loss,val_loss,val_metric))\n",
        "            \n",
        "    val_loss, val_metric = val_lossF(net, device, criterion, val_loader)  # Compute validation loss  \n",
        "    print(\"Epoch {} complete! Train Loss : {} , Validation Loss : {} , Validation Metric - Accuracy : {} \".format(ep+1, loss, val_loss, val_metric))\n",
        "    train_losses.append(loss)\n",
        "    val_losses.append(val_loss)  \n",
        "    val_metrics.append(val_metric)\n",
        "    if val_loss < best_loss:       \n",
        "        print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
        "        net_copy = copy.deepcopy(net)  # save a copy of the model\n",
        "        best_loss = val_loss\n",
        "        best_ep = ep + 1\n",
        "        path_to_model='{}_ep_{}_val_loss_{}.pt'.format(m_name, best_ep, round(best_loss, 5))\n",
        "        torch.save(net_copy.state_dict(), MODEL_SAVE_PATH + path_to_model)\n",
        "        print(\"The model has been saved in {}\".format(path_to_model))\n",
        "    # else:\n",
        "    #     es_count += 1\n",
        "    \n",
        "    # if early_stop and es_count>es_counts_MAX:\n",
        "    #     print('Early Stop Train in Epoch : {} '.format(ep+1))\n",
        "    #     break\n",
        "\n",
        "del loss\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1125/1125 [05:17<00:00,  3.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 complete! Train Loss : 0.7070322632789612 , Validation Loss : 0.8714851622581482 , Validation Metric - Accuracy : 0.4395 \n",
            "Best validation loss improved from inf to 0.8714851622581482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1125 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The model has been saved in bbc_2_nofre_ep_1_val_loss_0.87149.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1125/1125 [05:18<00:00,  3.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2 complete! Train Loss : 0.7141696214675903 , Validation Loss : 0.8659525094032288 , Validation Metric - Accuracy : 0.44 \n",
            "Best validation loss improved from 0.8714851622581482 to 0.8659525094032288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1125 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The model has been saved in bbc_2_nofre_ep_2_val_loss_0.86595.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1125/1125 [05:17<00:00,  3.54it/s]\n",
            "  0%|          | 0/1125 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3 complete! Train Loss : 0.6895807981491089 , Validation Loss : 0.8687757544517517 , Validation Metric - Accuracy : 0.44 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1125/1125 [05:18<00:00,  3.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4 complete! Train Loss : 0.6874521970748901 , Validation Loss : 0.8593240089416504 , Validation Metric - Accuracy : 0.446 \n",
            "Best validation loss improved from 0.8659525094032288 to 0.8593240089416504\n",
            "The model has been saved in bbc_2_nofre_ep_4_val_loss_0.85932.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOqc1UooqWFl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "0107e316-76e7-4f80-a857-6b54d158fa45"
      },
      "source": [
        "p1 = plt.plot(range(epochs),train_losses,'b--',label='train_loss')\n",
        "p2 = plt.plot(range(epochs),val_losses,'r--',label='validation_loss')\n",
        "p3 = plt.plot(range(epochs),val_metrics,'g--',label='validation_metric')\n",
        "plt.plot(range(epochs),train_losses,'bo-',range(epochs),val_losses,'r+-',range(epochs),val_metrics,'g^-')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss & metric')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('train loss = ', train_losses)\n",
        "print('val loss = ', val_losses)\n",
        "print('val metric = ', val_metrics)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxU5Z3v8c+vll5p9r1ZVcBmB1nMJREMV8cwBkeNS6JGkhgzLqPRJK9Lbpy4z+TemzHG63adRKMJxoWMiRN1SNyCuAIKCCiLyNKALI1A793V9dw/nip6bwro6u1836/XeXVVPadOPadO9fM9z1nNOYeIiARXqL0rICIi7UtBICIScAoCEZGAUxCIiAScgkBEJOAUBCIiAacgEBEJOAWBSDPMbKuZ/ff2rodIuikIREQCTkEgcgzMLNPM7jWzXYnhXjPLTJT1NbM/m9lBMztgZm+YWShR9j/MbKeZFZvZBjOb275zIlIr0t4VEOlkfgKcDkwGHPAn4Bbgn4EfAIVAv8S4pwPOzMYA1wPTnXO7zGwEEG7baos0Tz0CkWNzGXCHc26vc24fcDtwRaKsGhgEDHfOVTvn3nD+Yl41QCYw1syizrmtzrlP2qX2Ik1QEIgcm8HAtjrPtyVeA/g/wGbgL2a2xcwWAjjnNgPfB24D9prZU2Y2GJEOQkEgcmx2AcPrPB+WeA3nXLFz7gfOuZOA+cDNyX0BzrknnXNfTLzXAf+rbast0jwFgUjLomaWlRyA3wO3mFk/M+sL/BT4HYCZnWtmp5iZAYfwm4TiZjbGzL6c2KlcAZQD8faZHZHGFAQiLXsR33AnhyxgBbAG+BB4H7grMe4o4GWgBHgbeNA59xp+/8DPgP3AZ0B/4MdtNwsiLTPdmEZEJNjUIxARCTgFgYhIwCkIREQCTkEgIhJwne4SE3379nUjRoxo72qIiHQqK1eu3O+c69dUWacLghEjRrBixYr2roaISKdiZtuaK9OmIRGRgFMQiIgEnIJARCTgFAQiIgGnIBARCbhgBcFtt7V3DUREOpxgBcHtt7d3DUQ6B600BUqnO4/guC1b5v+edRZkZvohKwt69oRZsyAjAz75BGIxyM72Q26uLx8zxpcXF/v35eZCXp7/GwnOVygBcvvtCoMA6fqt2G231e8JvPxy43EefPD4p28GOTk+KEpKIB6HUKh26NkTRo3y5evXg3M+PKJRP+Tnw6RJvvzdd31ZRoYPqcxMGD4cxo/3465eXRtSyaAaPBiGDvXvO3jQv5abC927Q7dunSOobrtNjc6JiMehogKqqqCy0g/V1f73l53tV262b699varK/83Nhb59/ePVq/3f5ADw3HP+t5uTA+Xl0K8f9O7dOX5Tckw63f0Ipk2b5o77zGIzv1ZfXOwb7dJS/8/Rq5f/u3EjFBVBWZn/4ZeV+R/96NG+/O234cCB2n+2ykrf2J56qn+8bJmfdixWO+Tl+ca+qgrWrvV/43E/OOcb/UjEv15V1bpfFvjGICvLf0ZJif8O6gZVfj4MGuTrs3GjHy8ZVJGID6Fhw3xDs26dr2+yR5WZCZMn+2mUlcHWrf6zcnNrw2r0aN94xGJ+Grm5/jvr1s1/N9Gor1NNTe13UFXlx62u9o0QwKFD/rtPNmTJYcgQPx+7d/sh2ZBVVfnPHD/ev3/LFigs9GWxmP9bUwMzZ/p5/+gjXx6L+ddjMf++L3zBP1+zxk+/pqZ2iERg+nT/ePVq2L+/dtnW1PjvasIE/3jdOj8PztWOk5UFI0b48i1b/G/OudohI8P/Nmtq/LRravzrdX/PUP+1tmLmV3IGDfK/g08/rf1tZGX5Yfx4KCjw465f71dOuneHHj38fI0Z41diMjL8OH36+N+FpIWZrXTOTWuyLHBB0JHnNx73jUEyqEpKahvyigr48EPf4CaHigrfyPbv78ddurTxmuGQITBggG+E3nyztqFMBtXgwf4f89Ah2LChthFLNli5ub5uyYa5tYVC/nM6o1DIf//hsF9mlZX+N5YcMjLgpJN8+c6dfpnVDeLcXN9YhsP+uy8rqy0Lh31DO2WKD5wPPvDLIByuHfr1qy1/7z2/3MJh/zwchoEDfRAly83qB/2gQT6ow2FYtap+T/WGG+CRR3wdDh70PenkbzL5+xswwM//gQOwcmVtuNbU+GUaDtcG6vF8twMH+t+2cz6kkz3lrCy/kjFzZu1KyrZttSHTs6cPmvHj/XcUDvuA6tvXTzegFARJ2gRxYpKbIIqLfW+quNj/U4ZCvif1ySe1Palkr2rkSD/Ojh0+yCoqfIO5bp0fv6GRI/2aYrIxmznT/3N/9pn/Z6/bW4lEYMYM32vYs8cPybJkgzZxom8Eiorg8GHfmCTLolHfUEcifn7i8dry5Nptt26+HkFrQFprpSkW8ysZ27f7wCgqgs8/90O/fv63UVjoe9vJXnpyJWfkSL8cdu2Cjz+u7a0lV1SOt45mfpmOHOkDo6wM9u6t35vJyYG5c30vpajI1717dx8yPXv6AJw40b8WifjebbL3mk4n0IYpCKRj6+g9tSDqDCtNzvmVjf37/YpG3ZA5eNBvdnPOb/Jctao2ZMrLfdAUFPhw+fRT//5k0CQ3zR2PZG8wHPY9ktxcX7/Dh33QJDeZdu8Of/d3vryw0K8c9ehRu9msXz8YN86XR6N+/FDohP5XFATSsSkIpCOqrvbhsXu3b6wPHPDDwYO+lzNunA+W99+HzZv9uOXlfqiq8pvlSkv9Zr+iovq9meMRifiwSkMQaPe/tL9bb23vGog0Fo3WbgoqKGjdadfU+F5Aaak/yGLXLh8yyd5MVZXff1NaCg8/7He2J/e3JA8SuPXWVuu1qUcgItJZpGnTUMD2gImISEMKAhGRziJNm1EVBCIinUWajuRSEIiIBJyCQEQk4BQEIiIBpyAQEQk4BYGISMApCEREAk5BICIScAoCEZGAUxCIiAScgkBEJOAUBCIiAacgEBEJOAWBiEjAKQhERAJOQSAiEnAKAhGRgFMQiIgEnIJARCTgFAQiIgGX1iAws3PMbIOZbTazhU2UDzOz18zsAzNbY2bz0lkfERFpLG1BYGZh4AHgK8BY4OtmNrbBaLcAzzjnpgCXAg+mqz4iItK0dPYIZgCbnXNbnHNVwFPAeQ3GcUD3xOMewK401kdERJqQziDIB3bUeV6YeK2u24DLzawQeBH4p6YmZGZXm9kKM1uxb9++dNRVRCSw2ntn8deB3zjnhgDzgN+aWaM6Oececc5Nc85N69evX5tXUkSkK0tnEOwEhtZ5PiTxWl3fAZ4BcM69DWQBfdNYJxERaSCdQbAcGGVmI80sA78z+PkG42wH5gKYWQE+CLTtR0SkDaUtCJxzMeB6YAnwEf7ooHVmdoeZzU+M9gPgu2a2Gvg9sMA559JVJxERaSySzok7517E7wSu+9pP6zxeD8xKZx1ERKRl7b2zWERE2pmCQEQk4BQEIiIBpyAQEQk4BYGISMApCEREAk5BICIScAoCEZGAUxCIiAScgkBEJOAUBCIiAacgEBEJOAWBiEjAKQhERAJOQSAiEnAKAhGRgFMQiIgEnIJARCTgFAQiIgGnIBARCTgFgYhIwCkIREQCTkEgIhJwCgIRkYBTEIiIBJyCQEQk4BQEIiIBpyAQEQk4BYGISMApCEREAu6oQWBmfzWznnWe9zKzJemtlgTBokUwYgSEQv7vokXtXSORYIqkME5f59zB5BPn3Odm1j+NdZIAWLQIrr4aysr8823b/HOAyy5rv3qJBFEqQRA3s2HOue0AZjYccOmtlnQWzkFlJRw6BLt3w+HDfigu9n8HDYJIBLZvhzVrfMNfVgZ//jOUl9efVlkZ/PCHcOGFkJXVPvMjEkSpBMFPgGVm9jfAgC8BV6e1Vq1s0SL4yU98YzRsGNx9d9da64zHfaO6d69vkJNDcTHk5UG3blBUBO+9B6Wlfigr8+8ZPhx69YI9e+Cdd3yjXlUF1dV+GDLEN8pFRbB1q/+seNwHgEvD6sBnn0F2NoTD/u/s2TBqFJj5zysogEmT/KCwEGkdRw0C59x/mdlU4PTES993zu1Pb7VaT1ttgojH4eDB2iG5ZmwGvXtDRQW8+aZvoJNrxWVl0LOnD6eKCliyxDfEyca4qgr69IG+ff24H34INTV+cM5/ZiQCsdiJ1T0UgsxMX4dQqHYIh30d+vb15WVlkJHhh8xM3xCfdhoMHerLNm+GnBzIza0dJk2C/v19qJSU+Pnt3h3OOgt27mxcl5wcmDrVB8KBA7BxI7z2Wu3ya1jvadP891dV5b+Tk0+GU0/1nzt5ssJCJBXmmlmtM7NTnXMfJ0KgEefc+2mtWTOmTZvmVqxYUe+16upqCgsLqaioaDR+YaFvOBsy840OQDRau4mj7tquc368ZHmysUnHmnBTzHxjF4n4x9XVta8nh0jE1w98ed2y5HsjibiPx2tfrzuYtc381FVa6nsZdb9LMx98ubmNx4/FoLg4i6VLh7B2bZTt22HfPt9jKSz0IdRUIA4e7IOqvNzP65AhcNJJMHasD4qpU2u/P5GuzMxWOuemNVXWUo/gZvwmoH9roswBX26FurWKwsJC8vLyGDFiBNagVSstTX06DRuEZKOZleUfJ4OgbmMbjfqGKxSqXWutu0YdjfpNHKGQf3/y9bpDsqEPmqIi3yuoqvK9jPx8HwRNcc5RVFTEoEGFLFw4slF5PA47dsD77/ue0+bNPigGDfJB8dFHvoe2alX994VCMHCgD9Fo1Pde8vN9z2LKFDjzTB8mCgvpypoNAufc1WYWAm5xzr15PBM3s3OAXwJh4FfOuZ81KP8FcGbiaQ7Q3znXk2NUUVHRZAiAb2Cqqhq/JxLx256TjXV7rRkHWZ8+zTf8DZkZffr0Yd++fU2Wh0J+f8fw4XD++U1Po25YrFvng6JnTx8UL70E+/fDrl2NwyLZ88rIgB49asPitNPg7LN9L6N/fx/4Ip1Rs5uGjoxg9oFzbsoxT9gsDGwEzgIKgeXA151z65sZ/5+AKc65b7c03aY2DX300UcUFBQ0OX5Rkd8vEI/XvpZsNFJthKTjaGlZt4ZkWHzwgd+BHg77508/7YOiuLjpFQvwv6vsbB8W/frB9Okwd67vUYRCfjNUcnOkSFs73k1DSa+Y2YXAf7ijpUZ9M4DNzrktiUo8BZwHNBkEwNeBW49h+ilJNvapboKQYKvbs6jr9tvrP9++3Q+lpbBliw+KXbv8Du6iIv949Wr41a8aTz872+8wnzEDZs3yO+PLy2HiRL/folu39M6jSEOpBMH38PsLYmZWgT+E1Dnnuh/lffnAjjrPC4GZTY2YODdhJPBqM+VXkzhkddiwYSlUub5j2QRxPA4ePMiTTz7Jtddee0zvmzdvHk8++SQ9ex7b1rAFCxZw7rnn8rWvfe2Y3ietZ9gwPyRdc03jcT7/3AfChg3w/PP+8Nvdu31QHDgAf/kL/OlPjd+X3C81ZYrvReTl+f0bY8b4o6GmTFFYSOtK5fDRvDaox6XAYudcE8f3gHPuEeAR8JuG2qA+x+TgwYM8+OCDjYIgFosRiTT/Fb/44ovprpq0o169/DBuHFxwQdPjHD7sd2S/+qoPjG3bfHgUFfnQePxxP05DoZAPhjFj/KG8hw/DyJH+PIuJE31Y5B3Hf25XP+dGmnbUIDCzV5xzc4/2WhN2AkPrPB+SeK0plwLXHa0uqZozp/FrF18M117rj+yZN69x+YIFfti/HxquaL/+esuft3DhQj755BMmT55MNBolKyuLXr168fHHH7Nx40b+4R/+gR07dlBRUcGNN97I1YkTGUaMGMGKFSsoKSnhK1/5Cl/84hd56623yM/P509/+hPZKex9fOWVV/jhD39ILBZj+vTpPPTQQ2RmZrJw4UKef/55IpEIZ599Nj//+c959tlnuf322wmHw/To0YOlS5cedfqSXt27w8yZfmjOnj3w7rt+B/fGjbU9iwEDYNMmv2mq4Vna4Dc5DR/ud3SXlfmd2iNH+vMsJk70m6bq/sR02Y+OK90B3WwQmFkW/kievmbWC79JCKA7frPP0SwHRpnZSHwAXAp8o4nPORXoBbx9bFXvOH72s5+xdu1aVq1axeuvv87f//3fs3btWkaO9Ic5Pvroo/Tu3Zvy8nKmT5/OhRdeSJ8G26o2bdrE73//e/793/+diy++mD/84Q9cfvnlLX5uRUUFCxYs4JVXXmH06NF885vf5KGHHuKKK67gueee4+OPP8bMOHjQXyrqjjvuYMmSJeTn5x95TTq+AQNg/nw/NGfnTr+D+8MPa8NixAh/Yt6qVf7vhx82fl///n6/WSzmeyHJc1WSysrgxht9jyN5Tkok4gNk1CjfM9mzx+9/i0b9zvXkCYfJneTFxf6w62jUD5GI3/SVPIMc/N+QroXcpLYI6JZ6BN8Dvg8MBuqePHYYuP9oE3bOxczsemAJ/vDRR51z68zsDmCFc+75xKiXAk8d447oFrW0Bp+T03J5375H7wEczYwZM46EAMB9993Hc889B8COHTvYtGlToyAYOXIkkydPBuC0005j69atR/2cDRs2MHLkSEaPHg3AlVdeyQMPPMD1119PVlYW3/nOdzj33HM599xzAZg1axYLFizg4osv5oLmtlVIp5Sf74fEom7Srl3+0Nm1a+GTT/yJdoWF/ve+fXvjEEgqKvK96bYUDvuwCIX8Dvlk65A8xDs72/+vhkJ+vpInSyaHnj19EIXDftNb3TIzH67JAwJWrao9Zyj5d+hQ33uqqYEVK+qXhUL+PJMRI3wAJsuT5wglN9sNG+Yb7w8+aHz+0Nixvn7FxX55NDy3aNw4f+RZcTH84AeNz6wvK/M9hLQHgXPul8AvzeyfnHP/93gm7px7EXixwWs/bfD8tuOZdkeWW+fU2Ndff52XX36Zt99+m5ycHObMmdPkGdCZmZlHHofDYcqb6uunKBKJ8N577/HKK6+wePFi7r//fl599VUefvhh3n33XV544QVOO+00Vq5c2SiQpOsaPNgPzYXFiBF+bbOh3r39xQBranzPoabGr9VPmOAb4A8/9Du/k+XxuO8RTJ7sX1u50u84r6nxZTU1fv/FxIn++dtv+wYveR2rmhrfkI8b55+/9Zbf9JV8fzzuD/4YPdq/9vbbtSd7Jq8M0KePP5kwWZe6ZfG4H//w4dpLn9S9mkDyKgP79vnywkL/PSTDyDn/PUWjfn5LShp/Z8uWtbwsEuuFJ2T79hOfRlIqRw09ama3AMMSJ5mNAsY45/7cetXo3PLy8iguLm6y7NChQ/Tq1YucnBw+/vhj3nnnnVb73DFjxrB161Y2b97MKaecwm9/+1tmz55NSUkJZWVlzJs3j1mzZnHSSScB8MknnzBz5kxmzpzJSy+9xI4dOxQEcsTdd9ffBAG+B33ffS2veergtcaSgVJd7a/hlbyIY3W1D4/MTB8kZWV+v2QsVjtUV/sgy8ryYfXVr/pxGjqOAyiblVIQACuB/5Z4vhN4FlAQJPTp04dZs2Yxfvx4srOzGTBgwJGyc845h4cffpiCggLGjBnD6aef3sKUjk1WVhaPPfYYF1100ZGdxf/4j//IgQMHOO+886ioqMA5xz333APAj370IzZt2oRzjrlz5zJp0qRWq4t0fsnGXkcNnbjkJqjMTD+0ZMSIlsvvvbfpgL777hOu5hGpnFm8wjk3re4Zxma22jnXLq3IsZ5ZLF2LlrUEUWscNXSiZxZXmVk2iZvRmNnJQOWxVUFERI7XZZelt2eWShDcCvwXMNTMFgGzgAXpq5IkXXfddbz5Zv3r/d14441861vfaqcaiUhXlMqZxX81s/fxN6Yx4MbOdGOazuyBBx5o7yqISACkegpHPv5cgAzgDDPTQegiIl1EKpeYeBSYCKwDkhdzdsB/pLFeIiLSRlLZR3C6c25s2msiIiLtIpVNQ2+bmYJARKSLSiUInsCHwQYzW2NmH5rZmnRXrCvrlriY/K5du5q9p8CcOXNoeL5EQ/feey9ldc4ymTdvXqteTG7BggUsXry41aYnIh1TKpuGfg1cAXxI7T4CaQWDBw8+oYb23nvv5fLLLycncf9D3d9ARI5HKj2Cfc65551znzrntiWHtNfsRMyZ03h48EFfVlbWdPlvfuPL9+9vXHYUCxcurHeo52233cZdd93F3LlzmTp1KhMmTOBPTdyKauvWrYwfPx6A8vJyLr30UgoKCjj//PPrXXTummuuYdq0aYwbN45bb/V387zvvvvYtWsXZ555JmeeeSbg72+wP3FRknvuuYfx48czfvx47r333iOfV1BQwHe/+13GjRvH2WefnfLF7V555RWmTJnChAkT+Pa3v01lZeWReR87diwTJ07khz/8IQDPPvss48ePZ9KkSZxxxhkpTV9E2k8qPYIPzOxJ4D+pc0axc05HDSVccsklfP/73+e66/y9dZ555hmWLFnCDTfcQPfu3dm/fz+nn3468+fPx5LX0W3goYceIicnh48++og1a9YwderUI2V33303vXv3pqamhrlz57JmzRpuuOEG7rnnHl577TX69u1bb1orV67kscce491338U5x8yZM5k9eza9evXSfQ9EpJFUgiAbHwBn13mtYx8+2sY3JJgyZQp79+5l165d7Nu3j169ejFw4EBuuukmli5dSigUYufOnezZs4eBAwc2OY2lS5dyww03ADBx4kQmTpx4pOyZZ57hkUceIRaLsXv3btavX1+vvKFly5Zx/vnnH7kc9gUXXMAbb7zB/Pnzdd8DEWkklTOLdT2DFFx00UUsXryYzz77jEsuuYRFixaxb98+Vq5cSTQaZcSIEU3eh+BoPv30U37+85+zfPlyevXqxYIFC45rOkm674GINKSbw7WSSy65hKeeeorFixdz0UUXcejQIfr37080GuW1115jW1N3/KjjjDPO4MknnwRg7dq1rFnjD8w6fPgwubm59OjRgz179vDSSy8deU9z90H40pe+xB//+EfKysooLS3lueee40tf+tJxz1vd+x4A9e57cOjQIebNm8cvfvELVq9eDdTe9+COO+6gX79+7Nix47g/W0TSL5VNQ5KCcePGUVxcTH5+PoMGDeKyyy7jq1/9KhMmTGDatGmceuqpLb7/mmuu4Vvf+hYFBQUUFBRw2mmnATBp0iSmTJnCqaeeytChQ5k1a9aR91x99dWcc845DB48mNdee+3I61OnTmXBggXMmDEDgKuuuoopU6aktBmoKbrvgUjXdtT7EXQ0uh9BsGlZixyflu5HkPKmITP732Z2WuLxL1qrciIi0r6OZdPQe8CPzGwcsDxN9ZF2oPseiARbs0FgZv8IvOCcS+7pewF/Q5oDwKb0V03aiu57IBJsLW0aui4ZAmbWC/gL8CowBzg//VUTEZG20NKmoaiZ5QJ9gT8C/+ac+x2AmeW0ReVERCT9WgqCfwO24O9MtgbAzIYBVwIb0l81ERFpC81uGnLO/TswGBgAfAWYCrwEjAa+1ya1ExGRtGvx8FHnXE1iqHTO3eycG+ecu0I3rz8xneV+BCfqX/7lX1os72j1FQkqXWKiHbXG/QjqBsGLL75Iz549W6NqraK5IHDOEY/HO1x9RYKqSwbBnN/MaTQ8uNzfj6CsuqzJ8t+s+g0A+8v2Nyo7mq56P4I5c+Zw0003MW3aNAoKCli+fDkXXHABo0aN4pZbbjky3u9+9ztmzJjB5MmT+d73vkdNTQ0LFy6kvLycyZMnc9lll7F161bGjBnDN7/5TcaPH8+OHTvq1feJJ55g4sSJTJo0iSuuuOKo37mItB5da6gVdOX7EWRkZLBixQp++ctfct5557Fy5Up69+7NySefzE033cTevXt5+umnefPNN4lGo1x77bUsWrSIn/3sZ9x///2sWrUK8CG0adMmHn/8cU4//fR6n7Fu3Truuusu3nrrLfr27cuBAweOazmIyPE5ahCY2Y3AY0Ax8CtgCrDQOfeXNNftuL2+4PVmy3KiOS2W983p22J5U7ry/Qjmz58PwIQJExg3bhyDBg0C4KSTTmLHjh0sW7aMlStXMn36dMD3bPr379/ktIYPH94oBABeffVVLrrooiOB1rt37xbrJCKtK5Uewbedc780s78DeuHvX/xb/AlmktBV70eQHD8UCtV7bygUIhaL4Zzjyiuv5F//9V+P+tnJYBKRjiWVfQTJbRnzgN8659bVeU0SuvL9CFoyd+5cFi9ezN69ewE4cODAkXmNRqNUV1cfdRpf/vKXefbZZykqKjoyDRFpO6kEwUoz+ws+CJaYWR4QT2+1Op+m7kewYsUKJkyYwBNPPJHS/QhKSkooKCjgpz/9aZP3I/jGN77R5P0IkjuLk+rej2DmzJlH7keQDmPHjuWuu+7i7LPPZuLEiZx11lns3r37SP0mTpzIZZdd1uI0xo0bx09+8hNmz57NpEmTuPnmm9NSVxFp2lHvR2BmIWAysMU5d9DMegNDnHNr2qKCDel+BMGmZS1yfE70fgRfADYkQuBy4BbgUGtWUERE2k8qQfAQUGZmk4AfAJ8AT6QycTM7x8w2mNlmM1vYzDgXm9l6M1tnZk+mXHNpNddddx2TJ0+uNzz22GPtXS0RaSOpHDUUc845MzsPuN8592sz+87R3mRmYeAB4CygEFhuZs8759bXGWcU8GNglnPuczNr+rjDFDjnmj1GX1rWWe5H0NluqyrSWaTSIyg2sx/jDxt9IbHPIJrC+2YAm51zW5xzVcBTwHkNxvku8IBz7nMA59ze1KteKysri6KiIjUUXZhzjqKiIrKystq7KiJdTio9gkuAb+DPJ/gscSnq/5PC+/KBHXWeFwIzG4wzGsDM3sRf7vo259x/NZyQmV0NXA0wbNiwRh80ZMgQCgsL2bdvXwrVks4qKyuLIUOGtHc1RLqcowZBovFfBEw3s3OB95xzKe0jSPHzR+HvejYEWGpmE5xz9S5J6Zx7BHgE/FFDDScSjUYZOXJkK1VJRCRYjrppyMwuxt+4/iLgYuBdM2v62sn17QSG1nk+JPFaXYXA8865aufcp8BGfDCIiEgbSWXT0E+A6cnt92bWD3gZONr1k5cDo8xsJD4ALsVvYqrrj5xqta0AAA9NSURBVMDXgcfMrC9+U9GW1KsvIiInKpWdxaEGO3GLUnmfcy4GXA8sAT4CnnHOrTOzO8xsfmK0JUCRma0HXgN+5JwrOqY5EBGRE5JKj+C/zGwJ8PvE80uAF1OZuHPuxYbjOud+WuexA25ODCIi0g5S2Vn8IzO7EEhe5OYR59xz6a2WiIi0lZRuTOOc+wPwhzTXRURE2kGzQWBmxUBTZ2gZfqtO97TVSkRE2kyzQeCcy2vLioiISPvokjevFxGR1CkIREQCTkEgIhJwCgIRkYBTEIiIBJyCQEQk4BQEIiIBpyAQEQk4BYGISMApCEREAk5BICIScAoCEZGAUxCIiAScgkBEJOAUBCIiAacgEBEJOAWBiEjAKQhERAJOQSAiEnAKAhGRgFMQiIgEnIJARCTgFAQiIgGnIBARCTgFgYhIwCkIREQCTkEgIhJwCgIRkYBTEIiIBJyCQEQk4BQEIiIBpyAQEQk4BYGISMClNQjM7Bwz22Bmm81sYRPlC8xsn5mtSgxXpbM+IiLSWCRdEzazMPAAcBZQCCw3s+edc+sbjPq0c+76dNVDRERals4ewQxgs3Nui3OuCngKOC+NnyciIschnUGQD+yo87ww8VpDF5rZGjNbbGZDm5qQmV1tZivMbMW+ffvSUVcRkcBq753F/wmMcM5NBP4KPN7USM65R5xz05xz0/r169emFRQR6erSGQQ7gbpr+EMSrx3hnCtyzlUmnv4KOC2N9RERkSakMwiWA6PMbKSZZQCXAs/XHcHMBtV5Oh/4KI31ERGRJqTtqCHnXMzMrgeWAGHgUefcOjO7A1jhnHseuMHM5gMx4ACwIF31ERGRpplzrr3rcEymTZvmVqxY0d7VEBHpVMxspXNuWlNl7b2zWERE2pmCQEQk4BQEIiIBpyAQEQk4BYGISMApCEREAk5BICIScAoCEZGAUxCIiAScgkBEJOAUBCIiAacgEBEJOAWBiEjAKQhERAJOQSAiEnAKAhGRgFMQiIh0AruLdzP7N7P5rOSzVp+2gkBEpBO4c+mdLNu+jDv/dmerTztt9ywWEemK4vE4ZbEy4vE4VfEq9pXuY0/pHkoqSyiNlVJWVUZpdSmjeo8iFo+xYf8GPj34KeWxcsqry6mIVVBZU8npQ06nMlbJmr1r2H5wO1U1VVTHq6mqqcLhmDhgIpWxSjYWbWRv6V7KY+UAPLbqMf559j8zsNvAVpunwATB7uLdXPqHS3n6a0+36hcoJ0bLpePpSMvEOUdlTSWfl39OSVUJxVXFlFSWUFJdQo/MHkTDUfaW7mX9vvWUVZVRFiujvLqc8lg5o/qMIjuSzY5DO/jgsw+ojFVSVVNFVbyKqlgVY/uPJSOcwY5DO9hQtIFYPEZNvIZYPEbcxRnSfQhxF+dA+QEOVhzE0br3d39h0wsAhC1MjavBMEIWwsyIhCLsPLyTzEgmIQsRd/Ej76txNdz5tzt54O8faLW6BCYI6narWvML7GqcczjniJP44TkwM+IuTiweO1KWHC8SihAKhYjVxKiqqTryvnjc/82IZBC2MLF4jLLqMhyOuIv7fyoHt/3tNpZtW8aPX/4xt825rVF9cjNyiYQiVMYqqYhVNCrvltGNcCjs17JilY3K8zLzCFmI8upyqmqqGpV3z+yOmTVb3iOrBwBl1WVU11Q3W15aVUosHmu2vKSqhJp4Tb0yM6N7ZncAiiuL6/2zA4QsRF5mXrPl4VCYbhndADhceRjnXLPlhyoONapbJBQhNyO3Ufktr97CG9ve4OYlN3P3l++mPFbO2r1rKa0qpbS6lLLqMsqqyuid05v+uf0pqSrh9a2vUxGrOLIcKmsqye+ez4DcARyqOMSbO96kuqaa6ng1sXiMWDzGwG4DycvM43DlYT458Ak1rsb/vhK/j2go6n9zrdwAAxjG/vL95GXkUVVTRXFlMeFQmLCFyYpkEQ6FObn3yfTO7s2BsgNsP7SdjEgGGaEMMiOZZIYzOWP4GfTM6sne0r3sLtlNdjSb7Eg2OZEccjJymD54OrkZuRRXFlNZU0m3aDe6ZXYjLyOPbhnd6JPdh2g4ipm1WNfdxbs56b6Tjjyvqqlq9V6BNfzxdHTTpk1zK1asOKb37C7ezfB7h1Md9//I0VAUw/yPLRwlPy/fj1eym8pYZb0fXkY4gwG5A3A49pTsadRYZEYy6Z3dG+cc+8r2NWoMMsOZR/7Z95ftb/TPnBnOJDcjF4fzax11lofDkRnOJDOSCfh/9oaioSiRkM/zZNexrrCFCVkIh2uyoUp+DyKtLRKKkBPNIRqKcrDiIGZ+jTf5mxzWYxhDug/BOcdH+z8iEoqQEc4gGoqSEclgfL/xjOw1kspYJev2rSMrnEVWNIusSBbZkWwmDJhAfl4+VTVV7CzeSW40l9yMXLpldKNbtBuDuw+mZ1ZPIhYhGo6SHckmFOp8u0WvfeFafv3Br+u1PRnhDK6actUxrdSa2Urn3LSmygLRI7hz6Z2NGvfe2b0ByInmMGXQFAxj7d61HK48fCShDaNbRjcmD5wMwOrPVlNaXerLEuP0zOzJhAETMIwVu1ZQUVOBUfv+Pjl9GNdvHGbGO4Xv+LVK82WG0T+3PwX9CgB4c/ubvotodmQag/MGM7rPaACWblvq31VnDWJo96Gc0vsUalwNb25/s17dDGNEzxFH/pneKXynUfkpvU9haI+hlFWXsXzX8np1NzNG9xlNfl4+h6sO88HuD+qVGcbYfmPpn9ufgxUHWbNnTaPpTxgwgb45fSkqK2Lt3rVH3mdmrP5sNTsO76DG1RC2MEO7Dz3yXSdNHTyVvIw8dhXvYlPRpkbLdnr+dHKiOWw/tJ1PP/+0UfkXhnyBjEgGWw9uZdvBbY3Kvzjsi4RDYTYf2MzOwzsblc8eMRuAjUUb2V28u15ZOBTmi8O+CMD6fevZV7qvXnlGOIMvDP0CAB/u+ZAD5QfqlWdHs5mRPwOAVZ+tarTWnpeZx9RBUwFYuWslJVUl9cp7ZvVk0sBJALy38z3Kq+uvCPTJ6cP4/uMBeGvHW416NHV/e29se4O4i7Pqs1VsO7QNhyNsYWYPn81VU6/i/d3vkxPNITcjl5xoDjnRHIbkDWFoj6FEw76hz8vIIy8jj+5Z3cmJ5HTKRrcjervw7UYroFU1VbxV+FarfUaX7xEku1V1NytkR7LZcuOWdt/+GWRaLh2PlknX1lKPoMtH9p1L72y0OSa5s0Xaj5ZLx6NlElxdPgjaolslx07LpePRMgmuLr9pSEREAr5pSEREWqYgEBEJOAWBiEjAKQhERAJOQSAiEnCd7qghM9sHND5FNDV9gf2tWJ32pHnpeLrKfIDmpaM6kXkZ7pzr11RBpwuCE2FmK5o7fKqz0bx0PF1lPkDz0lGla160aUhEJOAUBCIiARe0IHikvSvQijQvHU9XmQ/QvHRUaZmXQO0jEBGRxoLWIxARkQYUBCIiAdclg8DMzjGzDWa22cwWNlGeaWZPJ8rfNbMRbV/L1KQwLwvMbJ+ZrUoMV7VHPY/GzB41s71mtraZcjOz+xLzucbMprZ1HVOVwrzMMbNDdZbJT9u6jqkws6Fm9pqZrTezdWZ2YxPjdIrlkuK8dJblkmVm75nZ6sS83N7EOK3bhiVvQt5VBiAMfAKcBGQAq4GxDca5Fng48fhS4On2rvcJzMsC4P72rmsK83IGMBVY20z5POAlwIDTgXfbu84nMC9zgD+3dz1TmI9BwNTE4zxgYxO/r06xXFKcl86yXAzolngcBd4FTm8wTqu2YV2xRzAD2Oyc2+KcqwKeAs5rMM55wOOJx4uBuVb3RsAdRyrz0ik455YCB1oY5TzgCee9A/Q0s0FtU7tjk8K8dArOud3OufcTj4uBj4D8BqN1iuWS4rx0ConvOnmD6mhiaHhUT6u2YV0xCPKBHXWeF9L4B3FkHOdcDDgE9GmT2h2bVOYF4MJEt32xmQ1tm6q1ulTntbP4QqJr/5KZjWvvyhxNYtPCFPzaZ12dbrm0MC/QSZaLmYXNbBWwF/irc67Z5dIabVhXDIKg+U9ghHNuIvBXatcSpP28j7+uyyTg/wJ/bOf6tMjMugF/AL7vnDvc3vU5EUeZl06zXJxzNc65ycAQYIaZjU/n53XFINgJ1F0rHpJ4rclxzCwC9ACK2qR2x+ao8+KcK3LOVSae/go4rY3q1tpSWW6dgnPucLJr75x7EYiaWd92rlaTzCyKbzgXOef+o4lROs1yOdq8dKblkuScOwi8BpzToKhV27CuGATLgVFmNtLMMvA7Up5vMM7zwJWJx18DXnWJvS4dzFHnpcH22vn4baOd0fPANxNHqZwOHHLO7W7vSh0PMxuY3F5rZjPw/2cdbkUjUcdfAx855+5pZrROsVxSmZdOtFz6mVnPxONs4Czg4wajtWobFjneN3ZUzrmYmV0PLMEfdfOoc26dmd0BrHDOPY//wfzWzDbjd/pd2n41bl6K83KDmc0HYvh5WdBuFW6Bmf0ef9RGXzMrBG7F7wTDOfcw8CL+CJXNQBnwrfap6dGlMC9fA64xsxhQDlzaQVc0ZgFXAB8mtkcD/E9gGHS65ZLKvHSW5TIIeNzMwviwesY59+d0tmG6xISISMB1xU1DIiJyDBQEIiIBpyAQEQk4BYGISMApCEREAk5BINKGElfA/HN710OkLgWBiEjAKQhEmmBmlyeuCb/KzP5f4iJgJWb2i8Q14l8xs36JcSeb2TuJC/89Z2a9Eq+fYmYvJy5y9r6ZnZyYfLfEBQI/NrNFHfTKtxIgCgKRBsysALgEmJW48FcNcBmQiz+zcxzwN/wZxQBPAP8jceG/D+u8vgh4IHGRs/8GJC/NMAX4PjAWf6+JWWmfKZEWdLlLTIi0grn4i/ctT6ysZ+MvBxwHnk6M8zvgP8ysB9DTOfe3xOuPA8+aWR6Q75x7DsA5VwGQmN57zrnCxPNVwAhgWfpnS6RpCgKRxgx43Dn343ovmv1zg/GO9/oslXUe16D/Q2ln2jQk0tgrwNfMrD+AmfU2s+H4/5evJcb5BrDMOXcI+NzMvpR4/Qrgb4m7ZBWa2T8kppFpZjltOhciKdKaiEgDzrn1ZnYL8BczCwHVwHVAKf4mIbfgNxVdknjLlcDDiYZ+C7VX6LwC+H+Jq0ZWAxe14WyIpExXHxVJkZmVOOe6tXc9RFqbNg2JiAScegQiIgGnHoGISMApCEREAk5BICIScAoCEZGAUxCIiATc/wdIwngLEo3yggAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "train loss =  [tensor(0.7070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), tensor(0.7142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), tensor(0.6896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), tensor(0.6875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)]\n",
            "val loss =  [0.8714851622581482, 0.8659525094032288, 0.8687757544517517, 0.8593240089416504]\n",
            "val metric =  [0.4395, 0.44, 0.44, 0.446]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0gMcF6BqWFm",
        "outputId": "55458761-8e9a-4bb0-ceda-3a0ad4e032af"
      },
      "source": [
        "net = TextSimilarityModel(dropout_rate=dropout_rate, finetune_units=finetune_units,bert_model=bert_model)\n",
        "net.load_state_dict(torch.load(MODEL_SAVE_PATH + path_to_model))\n",
        "net.to(device)\n",
        "\n",
        "test_set = LoadDataset(test, maxlen, with_labels=False, bert_model = bert_model)\n",
        "test_loader = DataLoader(test_set, batch_size=bs)\n",
        "\n",
        "net.eval()\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2 in tqdm(test_loader):\n",
        "        token_ids1, attn_masks1, token_type_ids1 = token_ids1.to(device), attn_masks1.to(device), token_type_ids1.to(device)\n",
        "        token_ids2, attn_masks2, token_type_ids2 =  token_ids2.to(device), attn_masks2.to(device), token_type_ids2.to(device)\n",
        "#         ot1,ot2 = net(token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2)\n",
        "#         cs=nn.CosineSimilarity()\n",
        "#         output = cs(ot1,ot2)\n",
        "        output = net(token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2)\n",
        "        output = output.sigmoid().cpu().numpy()\n",
        "        output = np.where(output>thres, 1, 0)\n",
        "        results += output.tolist()\n",
        "\n",
        "test['similarity'] = results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:13<00:00,  9.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "YtOT6Np9xR6F",
        "outputId": "3cde5216-b7e3-4167-d6c7-e52ddd18a2db"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_a</th>\n",
              "      <th>sentence_b</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2005 年末至 2009 年期间是例外，当时他效力于瑞典的卡斯塔德联队、塞尔维亚的查查克足...</td>\n",
              "      <td>例外情况发生于 2005 年末至 2009 年期间，当时他效力于瑞典的卡斯塔德联队、塞尔维亚...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tabaci 河是罗马尼亚 Leurda 河的支流。</td>\n",
              "      <td>Leurda 河是罗马尼亚境内 Tabaci 河的一条支流。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1993 年，他为 A 级的坎恩郡美洲狮队和 AA 级的波特兰海狗队效力。</td>\n",
              "      <td>1993 年，他为 A 级球队波特兰海狗队和 AA 级球队凯恩县美洲狮队效力。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Winarsky 是 IEEE、Phi Beta Kappa、ACM 和 Sigma Xi ...</td>\n",
              "      <td>温那斯基是 ACM、IEEE、Phi Beta Kappa 和 Sigma Xi 的成员。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1938 年，他成为英埃苏丹的政府人类学家，并领导对努巴的实地考察工作。</td>\n",
              "      <td>1938 年，他成为英埃苏丹政府的人类学家，并与努巴一起从事野外工作。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>《土地贝西之地》是贝西伯爵及其 1964 年管弦乐队的录音室专辑，该专辑的音乐由比利·拜尔斯...</td>\n",
              "      <td>《土地贝西之地》是比利·拜尔斯及其管弦乐队在 1964 年录制的录音室专辑，由贝西伯爵作曲和编曲。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>他们的任务是保护各种太空前哨不受敌舰群的攻击。</td>\n",
              "      <td>他们的使命是保护各个前哨免受敌人舰队的攻击。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>他的母亲伊丽莎白这边有康沃尔公国大法官威廉·莫当特爵士，以及民事诉讼法院首席书记约翰·莫当特。</td>\n",
              "      <td>他的母亲这边有康沃尔公国大法官伊丽莎白·威廉·莫当特爵士，以及民事诉讼法院首席书记约翰·莫当特。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>2014 年，该网站推出了用于搜索产品的 iOS 和 Android 应用程序；产品特色包括...</td>\n",
              "      <td>2014 年推出网站 iOS 和安卓应用程序，用于产品搜索，产品功能包括交互式视频 - 产品...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>在另一个方向上，假设 \"m\" 通过准素分解是最强大的</td>\n",
              "      <td>在另一个方向，假设“m”是质数，具有强大的因子分解。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             sentence_a  ... similarity\n",
              "0     2005 年末至 2009 年期间是例外，当时他效力于瑞典的卡斯塔德联队、塞尔维亚的查查克足...  ...          1\n",
              "1                            Tabaci 河是罗马尼亚 Leurda 河的支流。  ...          1\n",
              "2                 1993 年，他为 A 级的坎恩郡美洲狮队和 AA 级的波特兰海狗队效力。  ...          1\n",
              "3     Winarsky 是 IEEE、Phi Beta Kappa、ACM 和 Sigma Xi ...  ...          1\n",
              "4                  1938 年，他成为英埃苏丹的政府人类学家，并领导对努巴的实地考察工作。  ...          1\n",
              "...                                                 ...  ...        ...\n",
              "1995  《土地贝西之地》是贝西伯爵及其 1964 年管弦乐队的录音室专辑，该专辑的音乐由比利·拜尔斯...  ...          1\n",
              "1996                            他们的任务是保护各种太空前哨不受敌舰群的攻击。  ...          1\n",
              "1997    他的母亲伊丽莎白这边有康沃尔公国大法官威廉·莫当特爵士，以及民事诉讼法院首席书记约翰·莫当特。  ...          1\n",
              "1998  2014 年，该网站推出了用于搜索产品的 iOS 和 Android 应用程序；产品特色包括...  ...          1\n",
              "1999                         在另一个方向上，假设 \"m\" 通过准素分解是最强大的  ...          1\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_jROzVZknsd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
