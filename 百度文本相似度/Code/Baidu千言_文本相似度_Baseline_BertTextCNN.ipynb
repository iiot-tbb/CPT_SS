{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baidu千言-文本相似度-Baseline-BertTextCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DXlS3vq5iUL"
      },
      "source": [
        "\"\"\"\r\n",
        "记得每次训练更改新的model name用以分别保存模型参数文件\r\n",
        "可以通过dataset id来选择三个数据集中的一个\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "m_name = 'berttextcnn_crwe_z1' # model name, bbc = 'bert-base-chinese\r\n",
        "dsid = 2 # dataset id = ['/bq_corpus','/lcqmc','/paws-x-zh'], 千言文本相似度比赛三个数据集是分开记分的"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSeiAr4AqWFe"
      },
      "source": [
        "debug = 0\n",
        "seed = 225\n",
        "\n",
        "# Model hyperparameter\n",
        "device = 'cuda'\n",
        "bert_model = 'hfl/chinese-roberta-wwm-ext' # 'bert-base-chinese' # 'hfl/chinese-roberta-wwm-ext'\n",
        "freeze_bert = False\n",
        "maxlen = 128\n",
        "finetune_units = 768\n",
        "dropout_rate = 0.1\n",
        "\n",
        "#　Train Hyperparameter\n",
        "bs = 16\n",
        "lr = 2e-5 #1e-3 #2e-5\n",
        "if debug:\n",
        "    epochs = 4\n",
        "    num_warmup_steps = 0\n",
        "else:\n",
        "    epochs = 8\n",
        "    num_warmup_steps = 2\n",
        "es_counts_MAX = 3\n",
        "# Postprocess hyperparameter\n",
        "thres = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay5oQQtTrel4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGMGp4N0roaJ"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVEUNv1YqWFf"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "from scipy.spatial.distance import cosine\n",
        "import nltk\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "#float16和float32自动混合精度加速计算，官方文档：https://pytorch.org/docs/stable/amp.html\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.cuda.amp import GradScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajTqKw9qqWFf"
      },
      "source": [
        "def set_seed(seed = 42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    return seed\n",
        "\n",
        "seed = set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6weZKnqqBLyp"
      },
      "source": [
        "# PATH Info\r\n",
        "CURR_PATH = os.getcwd()\r\n",
        "ROOT_PATH = CURR_PATH + '/drive/MyDrive/Baidu_Qianyan'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riZKNTNpqWFg"
      },
      "source": [
        "def mkdir(path):\n",
        "\tfolder = os.path.exists(path)\n",
        "\tif not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
        "\t\tos.makedirs(path)            #makedirs 创建文件时如果路径不存在会创建这个路径\n",
        "\t\tprint('---  New Model Folder: {}  ---'.format(m_name))\n",
        " \n",
        "\telse:\n",
        "\t\tprint('---  Model Dir Exsiting!  ---')\n",
        "\n",
        "def read_tsv(input_file):\n",
        "    with open(input_file,\"r\",encoding=\"utf-8\") as file:\n",
        "        lines = []\n",
        "        for line in file:\n",
        "            if len(line.strip().split(\"\\t\")) != 1:\n",
        "                lines.append(line.strip().split(\"\\t\"))\n",
        "        df = pd.DataFrame(lines)\n",
        "    return df\n",
        "\n",
        "DATASET_PATH = ['/bq_corpus','/lcqmc','/paws-x-zh']\n",
        "dataset_path = DATASET_PATH[dsid]\n",
        "ROOT_PATH = '/content/drive/MyDrive/Baidu_Qianyan'\n",
        "DATA_PATH = ['/train.tsv','/dev.tsv','/test.tsv']\n",
        "MODEL_SAVE_PATH = ROOT_PATH + '/model' + dataset_path + '/' + m_name \n",
        "mkdir(MODEL_SAVE_PATH)     \n",
        "\n",
        "train = pd.DataFrame()\n",
        "dev = pd.DataFrame()\n",
        "test = pd.DataFrame()\n",
        "for data_path in DATA_PATH:\n",
        "    PATH = ''.join([ROOT_PATH,dataset_path])\n",
        "    PATH = ''.join([PATH,data_path])\n",
        "    df = read_tsv(PATH)\n",
        "    if data_path == '/train.tsv':\n",
        "        train = pd.concat([train,df],axis = 0)\n",
        "    if data_path == '/dev.tsv':\n",
        "        dev = pd.concat([dev,df],axis = 0)\n",
        "    if data_path == '/test.tsv':\n",
        "        test = pd.concat([test,df],axis = 0)\n",
        "\n",
        "## bq_corpus在20746行的格式有问题，以下方法无法读取\n",
        "# train = pd.DataFrame()\n",
        "# for dataset_path in DATASET_PATH:\n",
        "#     print(dataset_path)\n",
        "#     for data_path in DATA_PATH:\n",
        "#         PATH = ''.join([ROOT_PATH,dataset_path])\n",
        "#         PATH = ''.join([PATH,data_path])\n",
        "#         read_df = pd.read_csv(PATH, header=0, delimiter='\\t')\n",
        "#         train.append(read_df)\n",
        "\n",
        "train[[2]] = train[[2]].astype(int)\n",
        "dev[[2]] = dev[[2]].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IR9OBn1qWFh"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3q4rpX3qWFg"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkr43-q3rDw4"
      },
      "source": [
        "cols_dict=['sentence_a', 'sentence_b', 'similarity']\r\n",
        "train.columns = cols_dict\r\n",
        "dev.columns = cols_dict\r\n",
        "test.columns = cols_dict[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVpo0p5YqWFg"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SsBlNUyFgN2"
      },
      "source": [
        "train['len_a']=train['sentence_a'].map(lambda x: len(x))\r\n",
        "train['len_b']=train['sentence_b'].map(lambda x: len(x))\r\n",
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF2MJ2qSqWFh"
      },
      "source": [
        "if debug:\n",
        "    df_train = train.iloc[2000:20000,:].reset_index(drop = True)\n",
        "    df_val = train.iloc[:2000,:]\n",
        "else:\n",
        "    df_train = train\n",
        "    df_val = dev\n",
        "df_val.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LqQCqKteonu"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNmqGnLJqWFh"
      },
      "source": [
        "class LoadDataset(Dataset):\n",
        "    def __init__(self, data, maxlen, with_labels=True, bert_model='bert-base-chinese'):\n",
        "        self.data = data\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model,output_loading_info = False)  \n",
        "        self.maxlen = maxlen\n",
        "        self.with_labels = with_labels \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
        "        sent1 = str(self.data.loc[index,'sentence_a'])\n",
        "        sent2 = str(self.data.loc[index,'sentence_b'])\n",
        "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
        "        encoded_input1 = self.tokenizer(sent1,sent2, padding='max_length', truncation=True, max_length=self.maxlen, return_tensors='pt')\n",
        "        token_ids1 =  encoded_input1['input_ids'].squeeze(0) \n",
        "        attn_masks1 =  encoded_input1['attention_mask'].squeeze(0)  \n",
        "        token_type_ids1 =  encoded_input1['token_type_ids'].squeeze(0) \n",
        "\n",
        "        if self.with_labels:  # True if the dataset has labels\n",
        "            label = self.data.loc[index, 'similarity']\n",
        "            return token_ids1, attn_masks1, token_type_ids1, label\n",
        "        else:\n",
        "            return token_ids1, attn_masks1, token_type_ids1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFZkwPDNqWFi"
      },
      "source": [
        "def val_lossF(net, device, criterion, dataloader):\n",
        "    net.eval()\n",
        "    mean_loss = 0\n",
        "    count = 0\n",
        "    true_labelss = []\n",
        "    list_val_outputs = []\n",
        "    val_metric = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for  i, (token_ids1, attn_masks1, token_type_ids1,labels) in enumerate(dataloader):\n",
        "            token_ids1, attn_masks1, token_type_ids1 = token_ids1.to(device), attn_masks1.to(device), token_type_ids1.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            val_output = net(token_ids1, attn_masks1, token_type_ids1)\n",
        "            mean_loss += criterion(val_output, labels.float()).item()\n",
        "            count += 1\n",
        "\n",
        "            val_outputs = val_output.sigmoid().cpu().numpy()\n",
        "            val_outputs = np.where(val_outputs>thres, 1, 0)\n",
        "            list_val_outputs += val_outputs.tolist()\n",
        "            labelss = labels.cpu().numpy()\n",
        "            true_labelss += labelss.tolist()  \n",
        "        val_metric = accuracy_score(list_val_outputs,true_labelss)       \n",
        "    return mean_loss / count, val_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToUFshaakaYN"
      },
      "source": [
        "class BertTextCNN(nn.Module):\r\n",
        "    def __init__(self, dropout_rate=0.2, finetune_units=768, bert_model='bert-base-chinese', freeze_bert=False):\r\n",
        "        super(BertTextCNN, self).__init__()\r\n",
        "        self.bert_layer1 = BertModel.from_pretrained(bert_model,output_loading_info = False)\r\n",
        "        if bert_model == 'bert-base-chinese':\r\n",
        "            self.hidden_size = 768\r\n",
        "        elif bert_model == 'hfl/chinese-roberta-wwm-ext':\r\n",
        "            self.hidden_size = 768\r\n",
        "            \r\n",
        "        if freeze_bert:\r\n",
        "            for p in self.bert_layer.parameters():\r\n",
        "                p.requires_grad = False\r\n",
        "\r\n",
        "        self.dropout0 = nn.Dropout(p=dropout_rate)\r\n",
        "        # textcnn\r\n",
        "        channel_num = 1\r\n",
        "        filter_num=128\r\n",
        "        filter_sizes=[2,3,4]\r\n",
        "        pool_way='avg'\r\n",
        "        self.convs = nn.ModuleList(\r\n",
        "            [nn.Conv2d(channel_num, filter_num, (size, self.hidden_size)) for size in filter_sizes])\r\n",
        "        self.pool_way = pool_way\r\n",
        "        self.dropout1 = nn.Dropout(p=dropout_rate)\r\n",
        "        self.fc = nn.Linear(len(filter_sizes) * filter_num, 1)\r\n",
        "\r\n",
        "    @autocast()\r\n",
        "    def forward(self,  token_ids1, attn_masks1, token_type_ids1):\r\n",
        "        vecs1 = self.bert_layer1(token_ids1, attn_masks1, token_type_ids1)\r\n",
        "        x = self.dropout0(vecs1[0])\r\n",
        "        # textcnn\r\n",
        "        x = x.unsqueeze(1) # conv2d 需要接收 4维 的输入\r\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] \r\n",
        "        if self.pool_way == 'max':\r\n",
        "            x = [F.max_pool1d(item, item.size(2)).squeeze(2) for item in x]\r\n",
        "        elif self.pool_way == 'avg':\r\n",
        "            x = [F.avg_pool1d(item, item.size(2)).squeeze(2) for item in x]  \r\n",
        "        x = torch.cat(x, 1)\r\n",
        "        x = self.dropout1(x)\r\n",
        "        x = self.fc(x)\r\n",
        "\r\n",
        "        return x.squeeze(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OYxoyFxqWFk"
      },
      "source": [
        "device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "net = BertTextCNN(dropout_rate=dropout_rate, finetune_units=finetune_units,bert_model=bert_model,freeze_bert=freeze_bert)\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akpv2SPRqWFl"
      },
      "source": [
        "train_set = LoadDataset(df_train, maxlen, bert_model)\n",
        "val_set = LoadDataset(df_val, maxlen, bert_model)\n",
        "train_loader = DataLoader(train_set, batch_size=bs)\n",
        "val_loader = DataLoader(val_set, batch_size=bs)\n",
        "\n",
        "#criterion = nn.MSELoss()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
        "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps )\n",
        "scaler = GradScaler()\n",
        "\n",
        "best_loss = np.Inf\n",
        "best_metric = -np.Inf\n",
        "best_ep = 1\n",
        "iters = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_metrics = []\n",
        "es_count = 0\n",
        "for ep in range(epochs):\n",
        "    for it, (token_ids1, attn_masks1, token_type_ids1,labels) in tqdm(enumerate(train_loader), total = len(train_loader)):\n",
        "        net.train()\n",
        "        \n",
        "        token_ids1, attn_masks1, token_type_ids1 = token_ids1.to(device), attn_masks1.to(device), token_type_ids1.to(device)\n",
        "        labels = labels.to(device)\n",
        "        opti.zero_grad()\n",
        "        with autocast():\n",
        "            output = net(token_ids1, attn_masks1, token_type_ids1)\n",
        "            loss = criterion(output, labels.float())\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opti)\n",
        "        scaler.update()      \n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # if it % 100 == 0:\n",
        "        #     val_loss, val_metric = val_lossF(net, device, criterion, val_loader)  # Compute validation loss\n",
        "        #     print(\"it = {}, train_loss = {}, val_loss = {}, val_metric= {}\".format(it+1,loss,val_loss,val_metric))\n",
        "            \n",
        "    val_loss, val_metric = val_lossF(net, device, criterion, val_loader)  # Compute validation loss  \n",
        "    print(\"Epoch {} complete! Train Loss : {} , Validation Loss : {} , Validation Metric - Accuracy : {} \".format(ep+1, loss, val_loss, val_metric))\n",
        "    train_losses.append(loss)\n",
        "    val_losses.append(val_loss)  \n",
        "    val_metrics.append(val_metric)\n",
        "    # if val_loss < best_loss:       \n",
        "    #     print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
        "    #     net_copy = copy.deepcopy(net)  # save a copy of the model\n",
        "    #     best_loss = val_loss\n",
        "    #     best_ep = ep + 1\n",
        "    #     path_to_model='ep_{}_val_loss_{}.pt'.format(best_ep, round(best_loss, 4))\n",
        "    #     torch.save(net_copy.state_dict(), MODEL_SAVE_PATH + '/' + path_to_model)\n",
        "    #     print(\"The model has been saved in {}\".format(path_to_model))\n",
        "    if val_metric > best_metric:       \n",
        "        print(\"Best validation metric improved from {} to {}\".format(best_metric, val_metric))\n",
        "        net_copy = copy.deepcopy(net)  # save a copy of the model\n",
        "        best_metric = val_metric\n",
        "        best_ep = ep + 1\n",
        "        path_to_model='ep_{}_val_metric_{}.pt'.format(best_ep, round(best_metric, 4))\n",
        "        torch.save(net_copy.state_dict(), MODEL_SAVE_PATH + '/' + path_to_model)\n",
        "        print(\"The model has been saved in {}\".format(path_to_model))\n",
        "    # else:\n",
        "    #     es_count += 1\n",
        "    \n",
        "    # if early_stop and es_count>es_counts_MAX:\n",
        "    #     print('Early Stop Train in Epoch : {} '.format(ep+1))\n",
        "    #     break\n",
        "\n",
        "del loss\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOqc1UooqWFl"
      },
      "source": [
        "p1 = plt.plot(range(epochs),train_losses,'b--',label='train_loss')\n",
        "p2 = plt.plot(range(epochs),val_losses,'r--',label='validation_loss')\n",
        "p3 = plt.plot(range(epochs),val_metrics,'g--',label='validation_metric')\n",
        "plt.plot(range(epochs),train_losses,'bo-',range(epochs),val_losses,'r+-',range(epochs),val_metrics,'g^-')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss & metric')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('train loss = ', train_losses)\n",
        "print('val loss = ', val_losses)\n",
        "print('val metric = ', val_metrics)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0gMcF6BqWFm"
      },
      "source": [
        "net = BertTextCNN(dropout_rate=dropout_rate, finetune_units=finetune_units,bert_model=bert_model)\n",
        "net.load_state_dict(torch.load(MODEL_SAVE_PATH + '/' + path_to_model))\n",
        "net.to(device)\n",
        "\n",
        "test_set = LoadDataset(test, maxlen, with_labels=False, bert_model = bert_model)\n",
        "test_loader = DataLoader(test_set, batch_size=bs)\n",
        "\n",
        "net.eval()\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for token_ids1, attn_masks1, token_type_ids1 in tqdm(test_loader):\n",
        "        token_ids1, attn_masks1, token_type_ids1 = token_ids1.to(device), attn_masks1.to(device), token_type_ids1.to(device)\n",
        "        output = net(token_ids1, attn_masks1, token_type_ids1)\n",
        "        output = output.sigmoid().cpu().numpy()\n",
        "        output = np.where(output>thres, 1, 0)\n",
        "        results += output.tolist()\n",
        "\n",
        "test['similarity'] = results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtOT6Np9xR6F"
      },
      "source": [
        "test.to_csv(MODEL_SAVE_PATH+'dataset_path'+'preds.csv')\r\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}